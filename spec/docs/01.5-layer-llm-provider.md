# Layer 1.5: LLM Provider Abstraction - AI Agent Implementation Spec

## Overview

The LLM Provider is a **foundational service** required by Layers 3 (Reasoning), 4 (Verification), 5 (Cost), and 10 (Interaction). It provides a unified, Effect-TS-native interface for calling language models from any provider (Anthropic, OpenAI, local models) with built-in streaming, structured output parsing, token counting, and retry logic.

**This is a standalone package: `@reactive-agents/llm-provider`** — a cross-cutting dependency required by Layers 3-5, 7, and 10.

---

## Package Structure

```
@reactive-agents/llm-provider/
├── src/
│   ├── index.ts                  # Public API re-exports
│   ├── types.ts                  # LLM types & schemas
│   ├── llm-service.ts            # LLMService Context.Tag definition
│   ├── providers/
│   │   ├── anthropic.ts          # Anthropic Claude provider
│   │   ├── openai.ts             # OpenAI provider
│   │   └── local.ts              # Ollama / local model provider
│   ├── structured-output.ts      # Schema-based output parsing
│   ├── prompt-manager.ts         # Context window management
│   ├── token-counter.ts          # Token counting utilities
│   └── retry.ts                  # Retry & circuit breaker
```

---

## Core Types

```typescript
import { Effect, Context, Layer, Stream, Schema, Data } from "effect";

// ─── Embedding Configuration ───

/**
 * Configuration for the embedding sub-service.
 * Anthropic has no embeddings API. Embeddings are routed to OpenAI or Ollama.
 * This is the SOLE embedding source for the framework — `@reactive-agents/memory`
 * does NOT have its own EmbeddingProvider service.
 */
export interface EmbeddingConfig {
  /** Model name (e.g., "text-embedding-3-small") */
  readonly model: string;
  /** Vector dimensions matching the model (must match sqlite-vec column size in Tier 2) */
  readonly dimensions: number;
  /** Provider for embeddings ("openai" recommended; "ollama" for local) */
  readonly provider: "openai" | "ollama";
  /** Max texts per API call (default: 100) */
  readonly batchSize?: number;
}

export const DefaultEmbeddingConfig: EmbeddingConfig = {
  model: "text-embedding-3-small",
  dimensions: 1536,
  provider: "openai",
  batchSize: 100,
};

// ─── Model Configuration ───

export interface ModelConfig {
  readonly provider: LLMProvider;
  readonly model: string;
  readonly maxTokens?: number;
  readonly temperature?: number;
  readonly topP?: number;
  readonly stopSequences?: readonly string[];
}

export type LLMProvider = "anthropic" | "openai" | "ollama" | "custom";

export const ModelPresets = {
  // Fast, cheap — simple tasks
  "claude-haiku": {
    provider: "anthropic" as const,
    model: "claude-3-5-haiku-20241022",
    costPer1MInput: 1.0,
    costPer1MOutput: 5.0,
    maxContext: 200_000,
    quality: 0.6,
  },
  // Balanced — most tasks
  "claude-sonnet": {
    provider: "anthropic" as const,
    model: "claude-sonnet-4-20250514",
    costPer1MInput: 3.0,
    costPer1MOutput: 15.0,
    maxContext: 200_000,
    quality: 0.85,
  },
  // Balanced (newer) — most tasks, higher throughput
  "claude-sonnet-4-5": {
    provider: "anthropic" as const,
    model: "claude-sonnet-4-5-20250929",
    costPer1MInput: 3.0,
    costPer1MOutput: 15.0,
    maxContext: 200_000,
    quality: 0.9,
  },
  // Best quality — complex reasoning
  "claude-opus": {
    provider: "anthropic" as const,
    model: "claude-opus-4-20250514",
    costPer1MInput: 15.0,
    costPer1MOutput: 75.0,
    maxContext: 1_000_000,
    quality: 1.0,
  },
  // OpenAI options
  "gpt-4o-mini": {
    provider: "openai" as const,
    model: "gpt-4o-mini",
    costPer1MInput: 0.15,
    costPer1MOutput: 0.6,
    maxContext: 128_000,
    quality: 0.55,
  },
  "gpt-4o": {
    provider: "openai" as const,
    model: "gpt-4o",
    costPer1MInput: 2.5,
    costPer1MOutput: 10.0,
    maxContext: 128_000,
    quality: 0.8,
  },
} as const;

export type ModelPresetName = keyof typeof ModelPresets;

// ─── Message Types ───

export type LLMMessage =
  | { readonly role: "system"; readonly content: string }
  | {
      readonly role: "user";
      readonly content: string | readonly ContentBlock[];
    }
  | {
      readonly role: "assistant";
      readonly content: string | readonly ContentBlock[];
    };

// ─── Prompt Caching (Anthropic) ───

/**
 * Cache control for Anthropic prompt caching.
 * Attach to text content blocks to cache them across requests.
 * Anthropic caches the prefix up to and including the last cached block.
 */
export interface CacheControl {
  readonly type: "ephemeral";
}

export type ContentBlock =
  | {
      readonly type: "text";
      readonly text: string;
      readonly cache_control?: CacheControl; // Anthropic prompt caching
    }
  | { readonly type: "image"; readonly source: ImageSource }
  | {
      readonly type: "tool_use";
      readonly id: string;
      readonly name: string;
      readonly input: unknown;
    }
  | {
      readonly type: "tool_result";
      readonly tool_use_id: string;
      readonly content: string;
    };

/**
 * A text content block with prompt caching enabled.
 * Use for memory context injections, static system prompts, and tool definitions.
 * Non-Anthropic providers silently ignore `cache_control`.
 */
export type CacheableContentBlock = {
  readonly type: "text";
  readonly text: string;
  readonly cache_control: CacheControl;
};

/**
 * Helper — wrap text in a cacheable content block.
 * Non-Anthropic providers silently ignore `cache_control`.
 */
export const makeCacheable = (text: string): CacheableContentBlock => ({
  type: "text",
  text,
  cache_control: { type: "ephemeral" },
});

export interface ImageSource {
  readonly type: "base64" | "url";
  readonly media_type: "image/png" | "image/jpeg" | "image/gif" | "image/webp";
  readonly data: string;
}

// ─── Completion Types ───

export interface CompletionRequest {
  readonly messages: readonly LLMMessage[];
  readonly model?: ModelConfig;
  readonly maxTokens?: number;
  readonly temperature?: number;
  readonly stopSequences?: readonly string[];
  readonly tools?: readonly ToolDefinition[];
  readonly systemPrompt?: string;
}

export interface CompletionResponse {
  readonly content: string;
  readonly stopReason: StopReason;
  readonly usage: TokenUsage;
  readonly model: string;
  readonly toolCalls?: readonly ToolCall[];
}

export type StopReason =
  | "end_turn"
  | "max_tokens"
  | "stop_sequence"
  | "tool_use";

export interface TokenUsage {
  readonly inputTokens: number;
  readonly outputTokens: number;
  readonly totalTokens: number;
  readonly estimatedCost: number; // USD
}

export interface ToolDefinition {
  readonly name: string;
  readonly description: string;
  readonly inputSchema: Record<string, unknown>; // JSON Schema
}

export interface ToolCall {
  readonly id: string;
  readonly name: string;
  readonly input: unknown;
}

// ─── Streaming Types ───

export type StreamEvent =
  | { readonly type: "text_delta"; readonly text: string }
  | {
      readonly type: "tool_use_start";
      readonly id: string;
      readonly name: string;
    }
  | { readonly type: "tool_use_delta"; readonly input: string }
  | { readonly type: "content_complete"; readonly content: string }
  | { readonly type: "usage"; readonly usage: TokenUsage }
  | { readonly type: "error"; readonly error: string };

// ─── Structured Output ───

/**
 * Request that includes a schema for parsing the LLM response.
 * Uses Effect Schema for runtime validation.
 */
export interface StructuredCompletionRequest<A> extends CompletionRequest {
  readonly outputSchema: Schema.Schema<A>;
  readonly retryOnParseFail?: boolean; // default: true
  readonly maxParseRetries?: number; // default: 2
}
```

---

## Error Types

```typescript
import { Data } from "effect";

export class LLMError extends Data.TaggedError("LLMError")<{
  readonly message: string;
  readonly provider: LLMProvider;
  readonly cause?: unknown;
}> {}

export class LLMRateLimitError extends Data.TaggedError("LLMRateLimitError")<{
  readonly message: string;
  readonly provider: LLMProvider;
  readonly retryAfterMs: number;
}> {}

export class LLMTimeoutError extends Data.TaggedError("LLMTimeoutError")<{
  readonly message: string;
  readonly provider: LLMProvider;
  readonly timeoutMs: number;
}> {}

export class LLMParseError extends Data.TaggedError("LLMParseError")<{
  readonly message: string;
  readonly rawOutput: string;
  readonly expectedSchema: string;
}> {}

export class LLMContextOverflowError extends Data.TaggedError(
  "LLMContextOverflowError",
)<{
  readonly message: string;
  readonly tokenCount: number;
  readonly maxTokens: number;
}> {}

export type LLMErrors =
  | LLMError
  | LLMRateLimitError
  | LLMTimeoutError
  | LLMParseError
  | LLMContextOverflowError;
```

---

## LLM Service Definition

```typescript
/**
 * Core LLM service — all LLM interactions go through this.
 * Layers 3, 4, 5, and 10 depend on this.
 */
export class LLMService extends Context.Tag("LLMService")<
  LLMService,
  {
    /**
     * Complete a prompt (non-streaming).
     * Returns full response after generation completes.
     */
    readonly complete: (
      request: CompletionRequest,
    ) => Effect.Effect<CompletionResponse, LLMErrors>;

    /**
     * Stream a completion. Returns an Effect Stream of events.
     * Use for real-time UI updates (collaborative mode).
     */
    readonly stream: (
      request: CompletionRequest,
    ) => Effect.Effect<Stream.Stream<StreamEvent, LLMErrors>, LLMErrors>;

    /**
     * Complete with structured output.
     * Parses LLM response into a typed object using Effect Schema.
     * Retries with parse error feedback if parsing fails.
     */
    readonly completeStructured: <A>(
      request: StructuredCompletionRequest<A>,
    ) => Effect.Effect<A, LLMErrors>;

    /**
     * Generate embeddings for text.
     *
     * **This is the SOLE embedding source for the entire framework.**
     * `@reactive-agents/memory` does NOT have a separate EmbeddingProvider.
     * All embedding calls go through here, routed per `LLMConfig.embeddingConfig`.
     *
     * Anthropic has no embeddings API. The default routes to OpenAI
     * (`text-embedding-3-small`, 1536 dims). Override `embeddingConfig` to use
     * Ollama for fully local operation.
     *
     * Used by Layer 2 (Memory) Tier 2 for sqlite-vec KNN search.
     * Layer 2 Tier 1 (FTS5-only) does not call embed() at all.
     */
    readonly embed: (
      texts: readonly string[],
      model?: string,
    ) => Effect.Effect<readonly number[][], LLMErrors>;

    /**
     * Count tokens for a set of messages.
     * Used for context window management.
     */
    readonly countTokens: (
      messages: readonly LLMMessage[],
    ) => Effect.Effect<number, LLMErrors>;

    /**
     * Get current model configuration.
     */
    readonly getModelConfig: () => Effect.Effect<ModelConfig, never>;
  }
>() {}
```

---

## Provider Implementations

### Anthropic Provider

> **Prompt Caching:** When `config.supportsPromptCaching` is `true`, the Anthropic
> provider automatically wraps the injected memory context (system prompt sections
> derived from `MemoryBootstrapResult`) in `cache_control: { type: "ephemeral" }`
> blocks. This caches the memory prefix across requests in the same session,
> reducing costs by up to 90% on memory-heavy prompts. Cache entries persist for
> 5 minutes (Anthropic's TTL). Non-Anthropic providers receive the message unchanged
> and silently ignore `cache_control`.

```typescript
// File: src/llm/providers/anthropic.ts

import { Effect, Layer, Stream } from "effect";
import Anthropic from "@anthropic-ai/sdk";

export const AnthropicLLMServiceLive = Layer.effect(
  LLMService,
  Effect.gen(function* () {
    const config = yield* LLMConfig;
    const client = new Anthropic({ apiKey: config.anthropicApiKey });

    return LLMService.of({
      complete: (request) =>
        Effect.gen(function* () {
          const response = yield* Effect.tryPromise({
            try: () =>
              client.messages.create({
                model: request.model?.model ?? "claude-sonnet-4-20250514",
                max_tokens: request.maxTokens ?? 4096,
                temperature: request.temperature ?? 0.7,
                system: request.systemPrompt,
                messages: request.messages.map(toLLMMessage),
                stop_sequences: request.stopSequences
                  ? [...request.stopSequences]
                  : undefined,
                tools: request.tools?.map(toAnthropicTool),
              }),
            catch: (error) => toEffectError(error, "anthropic"),
          });

          return mapAnthropicResponse(response);
        }).pipe(
          Effect.retry(retryPolicy),
          Effect.timeout("30 seconds"),
          Effect.catchTag("TimeoutException", () =>
            Effect.fail(
              new LLMTimeoutError({
                message: "LLM request timed out",
                provider: "anthropic",
                timeoutMs: 30_000,
              }),
            ),
          ),
        ),

      stream: (request) =>
        Effect.succeed(
          Stream.async<StreamEvent, LLMErrors>((emit) => {
            const stream = client.messages.stream({
              model: request.model?.model ?? "claude-sonnet-4-20250514",
              max_tokens: request.maxTokens ?? 4096,
              temperature: request.temperature ?? 0.7,
              system: request.systemPrompt,
              messages: request.messages.map(toLLMMessage),
            });

            stream.on("text", (text) => {
              emit.single({ type: "text_delta", text });
            });

            stream.on("finalMessage", (message) => {
              emit.single({
                type: "content_complete",
                content: message.content
                  .filter(
                    (b): b is { type: "text"; text: string } =>
                      b.type === "text",
                  )
                  .map((b) => b.text)
                  .join(""),
              });
              emit.single({
                type: "usage",
                usage: {
                  inputTokens: message.usage.input_tokens,
                  outputTokens: message.usage.output_tokens,
                  totalTokens:
                    message.usage.input_tokens + message.usage.output_tokens,
                  estimatedCost: calculateCost(
                    message.usage.input_tokens,
                    message.usage.output_tokens,
                    request.model?.model ?? "claude-sonnet-4-20250514",
                  ),
                },
              });
              emit.end();
            });

            stream.on("error", (error) => {
              emit.fail(
                new LLMError({
                  message: error.message,
                  provider: "anthropic",
                  cause: error,
                }),
              );
            });
          }),
        ),

      completeStructured: (request) =>
        Effect.gen(function* () {
          const schemaDescription = Schema.format(request.outputSchema);

          const messagesWithFormat: LLMMessage[] = [
            ...request.messages,
            {
              role: "user" as const,
              content: `\nRespond with ONLY valid JSON matching this schema:\n${schemaDescription}\n\nNo markdown, no code fences, just raw JSON.`,
            },
          ];

          let lastError: unknown = null;
          const maxRetries = request.maxParseRetries ?? 2;

          for (let attempt = 0; attempt <= maxRetries; attempt++) {
            const response = yield* LLMService.complete({
              ...request,
              messages:
                attempt === 0
                  ? messagesWithFormat
                  : [
                      ...messagesWithFormat,
                      {
                        role: "assistant" as const,
                        content: String(lastError),
                      },
                      {
                        role: "user" as const,
                        content: `That response was not valid JSON. The parse error was: ${String(lastError)}. Please try again with valid JSON only.`,
                      },
                    ],
            });

            const parseResult = Schema.decodeUnknownEither(
              request.outputSchema,
            )(JSON.parse(response.content));

            if (parseResult._tag === "Right") {
              return parseResult.right;
            }

            lastError = parseResult.left;
          }

          return yield* Effect.fail(
            new LLMParseError({
              message: `Failed to parse structured output after ${maxRetries + 1} attempts`,
              rawOutput: String(lastError),
              expectedSchema: schemaDescription,
            }),
          );
        }),

      embed: (texts, model) =>
        Effect.tryPromise({
          try: async () => {
            // Anthropic has no embeddings API.
            // Route to embeddingConfig.provider (OpenAI default, or Ollama for local).
            const embeddingModel = model ?? config.embeddingConfig.model;
            const embProvider = config.embeddingConfig.provider;

            if (embProvider === "openai") {
              // OpenAI embeddings (text-embedding-3-small by default)
              const { OpenAI } = await import("openai");
              const openaiClient = new OpenAI({ apiKey: config.openaiApiKey });
              const batchSize = config.embeddingConfig.batchSize ?? 100;
              const results: number[][] = [];

              for (let i = 0; i < texts.length; i += batchSize) {
                const batch = texts.slice(i, i + batchSize);
                const response = await openaiClient.embeddings.create({
                  model: embeddingModel,
                  input: [...batch],
                  dimensions: config.embeddingConfig.dimensions,
                });
                results.push(...response.data.map((d) => d.embedding));
              }

              return results;
            }

            // Ollama embeddings (local — no API key needed)
            const endpoint = config.ollamaEndpoint ?? "http://localhost:11434";
            return Promise.all(
              [...texts].map(async (text) => {
                const res = await fetch(`${endpoint}/api/embed`, {
                  method: "POST",
                  headers: { "Content-Type": "application/json" },
                  body: JSON.stringify({
                    model: embeddingModel,
                    input: text,
                  }),
                });
                const data = await res.json();
                return data.embeddings[0] as number[];
              }),
            );
          },
          catch: (error) =>
            new LLMError({
              message: `Embedding failed: ${error}`,
              provider: "anthropic",
              cause: error,
            }),
        }),

      countTokens: (messages) =>
        Effect.tryPromise({
          try: () =>
            client.messages
              .countTokens({
                model: "claude-sonnet-4-20250514",
                messages: messages.map(toLLMMessage),
              })
              .then((r) => r.input_tokens),
          catch: (error) =>
            new LLMError({
              message: `Token counting failed: ${error}`,
              provider: "anthropic",
              cause: error,
            }),
        }),

      getModelConfig: () =>
        Effect.succeed({
          provider: "anthropic" as const,
          model: "claude-sonnet-4-20250514",
        }),
    });
  }),
);
```

---

## LLM Configuration Service

```typescript
/**
 * LLM configuration — provided via environment or config file.
 */
export class LLMConfig extends Context.Tag("LLMConfig")<
  LLMConfig,
  {
    readonly defaultProvider: LLMProvider;
    readonly defaultModel: string;
    readonly anthropicApiKey?: string;
    readonly openaiApiKey?: string;
    readonly ollamaEndpoint?: string;
    /**
     * Embedding configuration. Anthropic has no embeddings API;
     * embeddings route to OpenAI (default) or Ollama.
     * This is the SOLE embedding config for the entire framework.
     */
    readonly embeddingConfig: EmbeddingConfig;
    /**
     * Enable Anthropic prompt caching.
     * When true, memory context injections are wrapped in
     * `cache_control: { type: "ephemeral" }` blocks, cutting
     * costs by up to 90% on memory-heavy prompts.
     * Set false for OpenAI/Ollama providers (they ignore it anyway).
     */
    readonly supportsPromptCaching: boolean;
    readonly maxRetries: number;
    readonly timeoutMs: number;
    readonly defaultMaxTokens: number;
    readonly defaultTemperature: number;
  }
>() {}

export const LLMConfigFromEnv = Layer.succeed(
  LLMConfig,
  LLMConfig.of({
    defaultProvider: "anthropic",
    defaultModel: "claude-sonnet-4-20250514",
    anthropicApiKey: process.env.ANTHROPIC_API_KEY,
    openaiApiKey: process.env.OPENAI_API_KEY,
    ollamaEndpoint: process.env.OLLAMA_ENDPOINT ?? "http://localhost:11434",
    embeddingConfig: {
      model: process.env.EMBEDDING_MODEL ?? "text-embedding-3-small",
      dimensions: Number(process.env.EMBEDDING_DIMENSIONS ?? 1536),
      provider:
        (process.env.EMBEDDING_PROVIDER as "openai" | "ollama") ?? "openai",
      batchSize: 100,
    },
    supportsPromptCaching: (
      process.env.LLM_DEFAULT_MODEL ?? "claude-sonnet-4-20250514"
    ).startsWith("claude"),
    maxRetries: 3,
    timeoutMs: 30_000,
    defaultMaxTokens: 4096,
    defaultTemperature: 0.7,
  }),
);
```

---

## Prompt Manager (Context Window Management)

```typescript
/**
 * Manages context window budgets.
 * Ensures prompts don't exceed model limits.
 * Implements truncation strategies.
 */
export class PromptManager extends Context.Tag("PromptManager")<
  PromptManager,
  {
    /**
     * Build a prompt within token budget.
     * Automatically truncates conversation history if needed.
     */
    readonly buildPrompt: (options: {
      readonly systemPrompt: string;
      readonly messages: readonly LLMMessage[];
      readonly reserveOutputTokens: number;
      readonly maxContextTokens: number;
      readonly truncationStrategy: TruncationStrategy;
    }) => Effect.Effect<readonly LLMMessage[], LLMErrors>;

    /**
     * Check if messages fit within context window.
     */
    readonly fitsInContext: (
      messages: readonly LLMMessage[],
      maxTokens: number,
    ) => Effect.Effect<boolean, LLMErrors>;
  }
>() {}

export type TruncationStrategy =
  | "drop-oldest" // Remove oldest messages first
  | "summarize-middle" // Summarize middle messages, keep first and last
  | "sliding-window" // Keep last N messages
  | "importance-based"; // Keep messages scored as important
```

---

## Retry & Circuit Breaker

```typescript
import { Schedule, Effect } from "effect";

/**
 * Retry policy for LLM calls.
 * Handles rate limits with exponential backoff.
 */
export const retryPolicy = Schedule.intersect(
  Schedule.recurs(3),
  Schedule.exponential("1 second", 2.0),
).pipe(
  Schedule.whileInput<LLMErrors>(
    (error) =>
      error._tag === "LLMRateLimitError" || error._tag === "LLMTimeoutError",
  ),
);

/**
 * Circuit breaker state.
 * Opens after N consecutive failures, closes after cooldown.
 */
export interface CircuitBreakerConfig {
  readonly failureThreshold: number; // Open after N failures
  readonly cooldownMs: number; // Wait before trying again
  readonly halfOpenRequests: number; // Test requests in half-open
}

export const defaultCircuitBreakerConfig: CircuitBreakerConfig = {
  failureThreshold: 5,
  cooldownMs: 30_000, // 30 seconds
  halfOpenRequests: 1,
};
```

---

## Structured Output Parsing

```typescript
import { Schema } from "effect";

// ─── Common Schemas for Reasoning Strategies ───

/**
 * Schema for ReAct action parsing.
 */
export const ReActActionSchema = Schema.Struct({
  thought: Schema.String,
  action: Schema.optional(
    Schema.Struct({
      tool: Schema.String,
      input: Schema.Unknown,
    }),
  ),
  finalAnswer: Schema.optional(Schema.String),
  isComplete: Schema.Boolean,
});

export type ReActAction = Schema.Schema.Type<typeof ReActActionSchema>;

/**
 * Schema for plan generation.
 */
export const PlanSchema = Schema.Struct({
  goal: Schema.String,
  steps: Schema.Array(
    Schema.Struct({
      id: Schema.Number,
      description: Schema.String,
      tool: Schema.optional(Schema.String),
      dependsOn: Schema.optional(Schema.Array(Schema.Number)),
      estimatedDuration: Schema.optional(Schema.String),
    }),
  ),
});

export type Plan = Schema.Schema.Type<typeof PlanSchema>;

/**
 * Schema for reflection output.
 */
export const ReflectionSchema = Schema.Struct({
  taskAccomplished: Schema.Boolean,
  confidence: Schema.Number,
  strengths: Schema.Array(Schema.String),
  weaknesses: Schema.Array(Schema.String),
  needsRefinement: Schema.Boolean,
  refinementSuggestions: Schema.optional(Schema.Array(Schema.String)),
});

export type Reflection = Schema.Schema.Type<typeof ReflectionSchema>;

/**
 * Schema for strategy selection.
 */
export const StrategySelectionSchema = Schema.Struct({
  selectedStrategy: Schema.String,
  reasoning: Schema.String,
  confidence: Schema.Number,
  alternativeStrategies: Schema.Array(
    Schema.Struct({
      strategy: Schema.String,
      whyNot: Schema.String,
    }),
  ),
});

export type StrategySelection = Schema.Schema.Type<
  typeof StrategySelectionSchema
>;

/**
 * Schema for thought evaluation (Tree-of-Thought).
 */
export const ThoughtEvaluationSchema = Schema.Struct({
  score: Schema.Number,
  reasoning: Schema.String,
  strengths: Schema.Array(Schema.String),
  weaknesses: Schema.Array(Schema.String),
  shouldExpand: Schema.Boolean,
});

export type ThoughtEvaluation = Schema.Schema.Type<
  typeof ThoughtEvaluationSchema
>;

/**
 * Schema for task complexity analysis.
 */
export const ComplexityAnalysisSchema = Schema.Struct({
  score: Schema.Number,
  factors: Schema.Array(
    Schema.Struct({
      factor: Schema.String,
      weight: Schema.Number,
      reasoning: Schema.String,
    }),
  ),
  recommendedStrategy: Schema.String,
  recommendedModel: Schema.String,
});

export type ComplexityAnalysis = Schema.Schema.Type<
  typeof ComplexityAnalysisSchema
>;
```

---

## Test Utilities

```typescript
/**
 * Mock LLM service for testing.
 * Returns deterministic responses based on prompt patterns.
 */
export const TestLLMService = (
  responses: Record<string, string>,
): typeof LLMService.Service => ({
  complete: (request) =>
    Effect.gen(function* () {
      const lastMessage = request.messages[request.messages.length - 1];
      const content =
        typeof lastMessage.content === "string" ? lastMessage.content : "";

      // Match against registered patterns
      for (const [pattern, response] of Object.entries(responses)) {
        if (content.includes(pattern)) {
          return {
            content: response,
            stopReason: "end_turn" as const,
            usage: {
              inputTokens: content.length,
              outputTokens: response.length,
              totalTokens: content.length + response.length,
              estimatedCost: 0,
            },
            model: "test-model",
          };
        }
      }

      // Default response
      return {
        content: "Test response",
        stopReason: "end_turn" as const,
        usage: {
          inputTokens: 0,
          outputTokens: 0,
          totalTokens: 0,
          estimatedCost: 0,
        },
        model: "test-model",
      };
    }),

  stream: (request) =>
    Effect.succeed(
      Stream.make(
        { type: "text_delta" as const, text: "Test " },
        { type: "text_delta" as const, text: "response" },
        {
          type: "content_complete" as const,
          content: "Test response",
        },
        {
          type: "usage" as const,
          usage: {
            inputTokens: 0,
            outputTokens: 0,
            totalTokens: 0,
            estimatedCost: 0,
          },
        },
      ),
    ),

  completeStructured: (request) =>
    Effect.gen(function* () {
      const response = yield* LLMService.complete(request);
      const parsed = JSON.parse(response.content);
      return Schema.decodeUnknownSync(request.outputSchema)(parsed);
    }),

  embed: (texts) =>
    Effect.succeed(
      texts.map(() => new Array(768).fill(0).map(() => Math.random())),
    ),

  countTokens: (messages) =>
    Effect.succeed(
      messages.reduce(
        (sum, m) =>
          sum + (typeof m.content === "string" ? m.content.length / 4 : 100),
        0,
      ),
    ),

  getModelConfig: () =>
    Effect.succeed({
      provider: "anthropic" as const,
      model: "test-model",
    }),
});

/**
 * Create a test Layer for LLMService.
 */
export const TestLLMServiceLayer = (responses: Record<string, string> = {}) =>
  Layer.succeed(LLMService, LLMService.of(TestLLMService(responses)));
```

---

## Usage Examples

### Basic Completion

```typescript
const program = Effect.gen(function* () {
  const llm = yield* LLMService;

  const response = yield* llm.complete({
    messages: [
      { role: "system", content: "You are a helpful assistant." },
      { role: "user", content: "What is the capital of France?" },
    ],
    maxTokens: 100,
    temperature: 0.3,
  });

  console.log(response.content); // "Paris"
  console.log(response.usage); // { inputTokens: 20, outputTokens: 5, ... }
});

Effect.runPromise(program.pipe(Effect.provide(AnthropicLLMServiceLive)));
```

### Structured Output

```typescript
const program = Effect.gen(function* () {
  const llm = yield* LLMService;

  // Get a plan with guaranteed structure
  const plan = yield* llm.completeStructured({
    messages: [
      {
        role: "user",
        content: "Create a plan to research competitor frameworks.",
      },
    ],
    outputSchema: PlanSchema,
    retryOnParseFail: true,
  });

  // plan is fully typed: { goal: string, steps: [...] }
  console.log(`Plan: ${plan.goal}, ${plan.steps.length} steps`);
});
```

### Streaming (for Collaborative Mode)

```typescript
const program = Effect.gen(function* () {
  const llm = yield* LLMService;

  const eventStream = yield* llm.stream({
    messages: [{ role: "user", content: "Write a paragraph about AI agents." }],
  });

  // Process stream events
  yield* Stream.runForEach(eventStream, (event) =>
    Effect.sync(() => {
      if (event.type === "text_delta") {
        process.stdout.write(event.text);
      }
    }),
  );
});
```

### In Reasoning Strategy (Correct Pattern)

```typescript
// ✅ CORRECT: Pure function strategy using LLMService from context
import { Effect } from "effect";
import { LLMService } from "@reactive-agents/llm-provider";

export const executeReactive = (
  task: Task,
  memory: MemorySnapshot,
  config: ReactiveConfig
): Effect.Effect<ReasoningResult, ExecutionError, LLMService> =>
  Effect.gen(function* () {
    const llm = yield* LLMService;  // Get from context, not constructor

    // Use structured output for reliable parsing
    const action = yield* llm.completeStructured({
      messages: [
        { role: "system", content: "You are a reasoning agent..." },
        { role: "user", content: `Task: ${JSON.stringify(task.input)}` },
      ],
      outputSchema: ReActActionSchema,
    });

    // action is typed: { thought, action?, finalAnswer?, isComplete }
    if (action.isComplete && action.finalAnswer) {
      return buildResult([...], action.finalAnswer, "completed");
    }

    if (action.action) {
      const observation = yield* tools.execute(
        action.action.tool,
        action.action.input
      );
      // ... continue loop
    }
  });
```

---

## Dependencies

```json
{
  "dependencies": {
    "effect": "^3.10.0",
    "@anthropic-ai/sdk": "^0.39.0"
  },
  "optionalDependencies": {
    "openai": "^4.80.0"
  }
}
```

---

## Integration with Other Layers

| Layer                   | Uses LLMService For                                                              |
| ----------------------- | -------------------------------------------------------------------------------- |
| Layer 2 (Memory)        | `embed()` — Tier 2 KNN vector search via sqlite-vec (Tier 1 does not call embed) |
| Layer 3 (Reasoning)     | All strategy execution, strategy selection                                       |
| Layer 4 (Verification)  | Semantic entropy, fact decomposition, self-consistency, NLI                      |
| Layer 5 (Cost)          | Model routing (selects which model config to use)                                |
| Layer 7 (Orchestration) | Orchestrator planning, sub-agent spawning                                        |
| Layer 10 (Interaction)  | Adaptive mode selection, user preference analysis                                |

---

## Environment Variables

```bash
# Required (at least one LLM provider)
ANTHROPIC_API_KEY=sk-ant-...
# or
OPENAI_API_KEY=sk-...

# Embedding configuration
# OpenAI is the default embedding provider (Anthropic has no embeddings API)
OPENAI_API_KEY=sk-...           # Required if EMBEDDING_PROVIDER=openai (default)
EMBEDDING_PROVIDER=openai       # "openai" (default) or "ollama" (local)
EMBEDDING_MODEL=text-embedding-3-small  # Model name
EMBEDDING_DIMENSIONS=1536       # Must match model (3-small=1536, ada-002=1536, 3-large=3072)

# Fully local operation (no external embedding API)
# EMBEDDING_PROVIDER=ollama
# EMBEDDING_MODEL=nomic-embed-text
# EMBEDDING_DIMENSIONS=768
# OLLAMA_ENDPOINT=http://localhost:11434

# Optional
OLLAMA_ENDPOINT=http://localhost:11434
LLM_DEFAULT_MODEL=claude-sonnet-4-20250514
LLM_DEFAULT_TEMPERATURE=0.7
LLM_MAX_RETRIES=3
LLM_TIMEOUT_MS=30000
```

---

## Performance Targets

| Operation                  | Target     | Notes               |
| -------------------------- | ---------- | ------------------- |
| Completion (non-streaming) | <10s p95   | Depends on model    |
| Structured output parse    | <12s p95   | Includes retry      |
| Streaming first token      | <500ms p95 | Time-to-first-token |
| Token counting             | <50ms p95  | API call            |
| Embedding (single)         | <200ms p95 | API call            |
| Embedding (batch 100)      | <2s p95    | Batched API call    |

---

## Success Criteria

- ✅ Anthropic provider works (complete + stream + structured)
- ✅ OpenAI provider works as alternative
- ✅ Structured output parsing with retry
- ✅ Token counting before send
- ✅ Context window overflow prevention
- ✅ Retry with exponential backoff on rate limits
- ✅ Test mock provides deterministic responses
- ✅ All reasoning strategies use LLMService (not raw fetch)
- ✅ Cost tracking per request (input/output tokens + USD)
- ✅ `embed()` routes to OpenAI or Ollama (not Nomic) per `embeddingConfig`
- ✅ `CacheableContentBlock` + `makeCacheable()` exported for memory layer use
- ✅ Prompt caching activated when `supportsPromptCaching: true` in LLMConfig
- ✅ `LLMConfig` has no `nomicApiKey` (removed)

---

## Build Order

1. `src/types.ts` — ModelConfig, EmbeddingConfig, DefaultEmbeddingConfig, CompletionRequest, CompletionResponse, LLMUsage, CacheControl, CacheableContentBlock, makeCacheable schemas
2. `src/errors.ts` — All error types (LLMError, ProviderError, TokenLimitError, StructuredOutputError, RateLimitError)
3. `src/llm-service.ts` — LLMService Context.Tag definition (complete, stream, completeStructured, embed, countTokens)
4. `src/token-counter.ts` — Token counting utilities
5. `src/structured-output.ts` — Schema-based output parsing with retry
6. `src/retry.ts` — Retry with exponential backoff + circuit breaker
7. `src/prompt-manager.ts` — Context window management (trim/truncate to fit)
8. `src/providers/anthropic.ts` — Anthropic Claude provider (Layer.effect implementing LLMService)
9. `src/providers/openai.ts` — OpenAI provider (Layer.effect implementing LLMService)
10. `src/providers/local.ts` — Ollama / local model provider (Layer.effect implementing LLMService)
11. `src/testing.ts` — TestLLMService mock with deterministic responses
12. `src/index.ts` — Public re-exports
13. Tests for each module

**Build this BEFORE Layer 3 (Reasoning).** The LLM provider is required by the reasoning engine.

---

**Status: CRITICAL — Implement immediately after core types (Layer 1)**
